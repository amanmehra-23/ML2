{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38753936",
   "metadata": {},
   "source": [
    "## AND NOT OR NAND NOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4d329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AND gate final weights and bias:\n",
      "Weights: [5.41303291 5.4126457 ]\n",
      "Bias: -8.21153834545523\n",
      "\n",
      "OR gate final weights and bias:\n",
      "Weights: [6.17254551 6.17273233]\n",
      "Bias: -2.8399603409500824\n",
      "\n",
      "NAND gate final weights and bias:\n",
      "Weights: [-5.4740388  -5.47367243]\n",
      "Bias: 8.30282471562294\n",
      "\n",
      "NOR gate final weights and bias:\n",
      "Weights: [-6.16385936 -6.16412937]\n",
      "Bias: 2.8355604494931637\n",
      "\n",
      "NOT gate final weights and bias:\n",
      "Weights: [-6.47355886]\n",
      "Bias: 3.1285968167966742\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_dim=2, learning_rate=0.1):\n",
    "        self.weights = np.random.randn(input_dim)  \n",
    "        self.bias = np.random.randn()              \n",
    "        self.lr = learning_rate                    \n",
    "\n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights) + self.bias\n",
    "        return sigmoid(summation)\n",
    "\n",
    "    def train(self, inputs, targets, epochs=10000):\n",
    "        for _ in range(epochs):\n",
    "            for input_vec, target in zip(inputs, targets):\n",
    "                output = self.predict(input_vec)\n",
    "                error = target - output\n",
    "                adjustment = error * sigmoid_derivative(output)\n",
    "                self.weights += self.lr * adjustment * np.array(input_vec)\n",
    "                self.bias += self.lr * adjustment\n",
    "        return self.weights, self.bias\n",
    "\n",
    "# Logical gates training data\n",
    "logic_gates = {\n",
    "    \"AND\": {\n",
    "        \"inputs\": np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n",
    "        \"targets\": np.array([0, 0, 0, 1])\n",
    "    },\n",
    "    \"OR\": {\n",
    "        \"inputs\": np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n",
    "        \"targets\": np.array([0, 1, 1, 1])\n",
    "    },\n",
    "    \"NAND\": {\n",
    "        \"inputs\": np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n",
    "        \"targets\": np.array([1, 1, 1, 0])\n",
    "    },\n",
    "    \"NOR\": {\n",
    "        \"inputs\": np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n",
    "        \"targets\": np.array([1, 0, 0, 0])\n",
    "    },\n",
    "    \"NOT\": {\n",
    "        \"inputs\": np.array([[0], [1]]),\n",
    "        \"targets\": np.array([1, 0])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and print final weights and bias for each gate\n",
    "for gate, data in logic_gates.items():\n",
    "    input_dim = data['inputs'].shape[1]\n",
    "    perceptron = Perceptron(input_dim=input_dim)\n",
    "    final_weights, final_bias = perceptron.train(data['inputs'], data['targets'])\n",
    "    print(f\"\\n{gate} gate final weights and bias:\")\n",
    "    print(f\"Weights: {final_weights}\")\n",
    "    print(f\"Bias: {final_bias}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef9dd1",
   "metadata": {},
   "source": [
    "## XOR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa0092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XOR gate final weights and biases:\n",
      "Weights (Input to Hidden): \n",
      "[[-4.77327517 -5.76147602]\n",
      " [ 5.02444039  5.77923941]]\n",
      "Bias (Hidden Layer): \n",
      "[ 2.34798436 -3.26468028]\n",
      "Weights (Hidden to Output): \n",
      "[[-7.20468233]\n",
      " [ 7.45516139]]\n",
      "Bias (Output Layer): \n",
      "[3.3588605]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Multi-layer perceptron class\n",
    "class XORPerceptron:\n",
    "    def __init__(self, input_dim=2, hidden_dim=2, learning_rate=0.1):\n",
    "        # Initialize weights and biases for the hidden and output layers\n",
    "        self.weights_input_hidden = np.random.randn(input_dim, hidden_dim)\n",
    "        self.bias_hidden = np.random.randn(hidden_dim)\n",
    "        self.weights_hidden_output = np.random.randn(hidden_dim, 1)\n",
    "        self.bias_output = np.random.randn(1)\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # Forward pass\n",
    "        hidden_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "        final_input = np.dot(hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        final_output = sigmoid(final_input)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def train(self, inputs, targets, epochs=10000):\n",
    "        for _ in range(epochs):\n",
    "            for input_vec, target in zip(inputs, targets):\n",
    "                # Forward pass\n",
    "                hidden_input = np.dot(input_vec, self.weights_input_hidden) + self.bias_hidden\n",
    "                hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "                final_input = np.dot(hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "                final_output = sigmoid(final_input)\n",
    "\n",
    "                # Backward pass (error calculation and weight updates)\n",
    "                error = target - final_output\n",
    "                output_adjustment = error * sigmoid_derivative(final_output)\n",
    "\n",
    "                hidden_error = output_adjustment.dot(self.weights_hidden_output.T)\n",
    "                hidden_adjustment = hidden_error * sigmoid_derivative(hidden_output)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.weights_hidden_output += self.lr * hidden_output.reshape(-1, 1) * output_adjustment\n",
    "                self.bias_output += self.lr * output_adjustment\n",
    "\n",
    "                self.weights_input_hidden += self.lr * input_vec.reshape(-1, 1) * hidden_adjustment\n",
    "                self.bias_hidden += self.lr * hidden_adjustment\n",
    "\n",
    "        return self.weights_input_hidden, self.bias_hidden, self.weights_hidden_output, self.bias_output\n",
    "\n",
    "# XOR gate inputs and targets\n",
    "xor_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_targets = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Train and test the XOR gate\n",
    "xor_perceptron = XORPerceptron()\n",
    "final_weights_input_hidden, final_bias_hidden, final_weights_hidden_output, final_bias_output = xor_perceptron.train(xor_inputs, xor_targets)\n",
    "\n",
    "print(\"\\nXOR gate final weights and biases:\")\n",
    "print(f\"Weights (Input to Hidden): \\n{final_weights_input_hidden}\")\n",
    "print(f\"Bias (Hidden Layer): \\n{final_bias_hidden}\")\n",
    "print(f\"Weights (Hidden to Output): \\n{final_weights_hidden_output}\")\n",
    "print(f\"Bias (Output Layer): \\n{final_bias_output}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f843f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
